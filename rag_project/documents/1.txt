TIMESTAMP:2025-05-29T14:58:04.198309
PROJECT REPORT
Medicheck AI
Submitted By
Jawad Ahmad
2021-uam-4605
2021-2025
Rashid Muneer
2021-uam-4605
2021-2025
Farooq Yousaf
2021-uam-4650
2021-2025
Supervised By
Dr. Nadeem Iqbal Kajla

INSTITUTE OF COMPUTING
MNS-UNIVERSITY OF AGRICULTURE, MULTAN PAKISTAN


○	FINAL APPROVAL
This is to certify that we have read this report submitted by Jawad Ahmad 2021-uam-4605, Rashid Muneer 4604, Farooq Yousaf 2021-uam-4650 and it is our judgment that this report is of sufficient standard to warrant its acceptance by MNS-University of Agriculture, Multan for the degree of BS (Data Science).
●	Committee:
    1.	External Examiner                         	______________________	
Examiner Name
 Designation
 Organization

    2.	Supervisor      	                        	________________	
Dr. Nadeem Iqbal Kajla
 Assistant Professor,
 Institute of Computing
.                                                                                                                   	
    3.   Director of Institute    	         	________________
Dr. Salman Qadri
 Director,
Institute of Computing
 MNS-University of Agriculture, Multan


DECLARATION
This is to certify that Jawad Ahmad (2021-uam-4605), Rashid Muneer (2021-uam-4604), Farooq Yousaf(2021-uam-4650) (, Session (2021-2025) have worked on and completed their software project “Medicheck Ai” at the , MNS-University of Agriculture, Multan, in partial fulfillment of the requirements for the degree of BS (Data Science).
●	Date:_____________
 
●	Signature:  _____________
Jawad Ahmad
Reg. No. 2021-Uam-4605
●	Signature:  _____________
Rashid Muneer
Reg. No. 2021-Uam-4604
●	Signature:  _____________
Farooq Yousaf
Reg. No. 2021-Uam-4650
 
DEDICATION 
All praise be to ALLAH, who bestowed upon me the strength and guidance. I commit this venture to my parents, siblings, companions and teachers.
 
○	ACKNOWLEDGMENT

In the name of ALLAH, the Most Gracious, the Most Merciful. I am deeply grateful to Allah Almighty for granting me the strength, patience, and determination to complete this project successfully. I would like to extend my heartfelt appreciation to my respected supervisor, Dr. Nadeem Iqbal Kajla, for his invaluable guidance, insightful feedback, and continuous encouragement throughout this journey. His expertise and mentorship were instrumental in shaping the direction and quality of this work. I am also sincerely thankful to my teammates and friends whose collaboration, discussions, and support significantly contributed to the development and refinement of this project. A special thanks to my family, whose unwavering belief in me and constant motivation helped me stay focused and determined, even during challenging times. Lastly, I acknowledge the countless academic resources, research studies, and development tools that laid the foundation for this project. Without the combined support and contributions of all these individuals and resources, the successful completion of this endeavor would not have been possible. Thank you!
 
○	PROJECT BRIEF
 
PROJECT NAME	MediCheck Ai 
UNIVERSITY NAME	MNS University of Agriculture Multan 
UNDERTAKEN BY	Jawad Ahmad,
Rashid Muneer,
Farooq Yousaf
SUPERVISED BY	Dr. Nadeem Iqbal Kajla
STARTING DATE	January 27, 2025
COMPLETION DATE	May 19, 2025
COMPUTER USED	Intel(R) Core(TM) i7-10700 CPU @ 2.90GHz   2.90 GHz
OPERATING SYSTEM	Windows 11 Pro
SOURCE LANGUAGE(S)	Flutter-Dart, Python Node js
DBMS USED	Firebase(Firestore)
TOOLS/PACKAGES	VS Code, Google Api, Firebase Cloud, Cloud messaging, Groq api, ollama server, llama vision 3.2 model

 
PLAGIARISM UNDERTAKING 

I solemnly declare that the work presented in the report titled “Medicheck Ai” is solely our work with no significant contribution from any other person. Small contribution/help wherever taken has been duly acknowledged and that complete report has been written by us.
I understand the zero-tolerance policy of the HEC and MNS-University of Agriculture, Multan towards plagiarism. Therefore, I as an Author of the above titled report declare that no portion of my report has been plagiarized and any material used as reference is properly referred/cited.
I undertake that if I am found guilty of any formal plagiarism in the above titled report even after award of the degree, the University reserves the rights to withdraw/revoke my degree and that HEC and the University has the right to publish my name on the HEC/University Website on which names of students are placed who submitted plagiarized report.
 
 
Student /Author Signature:______________
 
                                                             Name:__________________ 
○	ABSTRACT


Medicheck AI is a cross-platform mHealth application for delivering proactive, personalized healthcare support through integrating state-of-the-art artificial intelligence technologies. System architecture features a secure Firebase backend, which manages user data, authentication, and cloud storage and preserves end-to-end encryption as well as complies with standards of data privacy.At the heart of the app is an AI-driven med chatbot on the basis of LLMs for adaptive context-aware natural language processing complemented with voice recognition to facilitate hands-free use.Additional capabilities include intelligent medication scheduling with context-sensitive reminders and document interpretation to scan and read medical documents.
The system is formulated specifically to meet Pakistani customers' language and sociocultural needs with multilinguality in a bid to achieve maximum access and use. Through symptom sorting at an initial stage, real-time health guidance, and emergency information provision, Medicheck AI enhances everyday self-care conduct and optimizes health literacy among low-resource communities.
This research demonstrates the phenomenal potential of AI in mobile health technology, especially for disadvantaged populations.It also outlines a scalable framework for future expansion, including clinical validation studies, EHR interoperability, and ethically rooted AI deployment.Medicheck AI is thus a significant milestone toward democratization of digital health access through smart, culture-specific solutions. 
○	TABLE OF CONTENTS

Contents 	 Page No.
Chapter 1
Introduction
1.1	 Introduction	2
1.2	HealthCare Context	3
Chapter2
Methodology
2.1 Introduction	6
2.2 Mobile Application (Frontend)	7
         2.2.1 User Interface (UI/UX)	7
         2.2.2 Voice Assistant Integration	8
         2.2.3 Document Scanner	9
2.3 Backend Services (Firebase Cloud Backend)	11
         2.3.1 Authentication	11
         2.3.2 Cloud Firestore Database	11
         2.3.3 Firebase Cloud Functions	12
         2.3.4 Firebase Cloud Messaging (FCM)	12
         2.3.5 Backend Compliance and Reliability	12
2.4 AI & Machine Learning Integrations	13
         2.4.1 Symptom Checker	13
         2.4.2 Medical Report Analysis	14
         2.4.3 Voice Assistant	14
2.5 Voice Assistant (Flutter Implementation)	16
        2.5.1 Introduction	16
        2.5.2 Technologies Used	16
        2.5.3 Voice Interaction Workflow	16
        2.5.4 Example Use Case	17
        2.5.5 Technical Challenges and Solutions	18
        2.5.6 Privacy Considerations	18
2.6 Medicine Reminder System	18
        2.6.1 Importance of Medication Adherence	18
        2.6.2 Setting Up Medications	19
        2.6.3 Data Storage in Firestore	20
        2.6.4 Notification Mechanism 	20
        2.6.5 Customization and Flexibility	20
       2.6.6 Effectiveness and Clinical Relevance	21
       2.6.7 Security and Privacy Considerations	21
       2.6.8 Planned Enhancements	21
       2.6.9 Real-World Example	21
       2.7 Software Engineering Model Used  	22
      2.8 Security & Compliance	22
      2.8.1 Data Encryption	23
     2.8.2 User Authentication & Access Control	23
     2.8.3 Ethical Handling of Medical Data	24
	29
	29
	29
	30
	30
	30

	              32
	32
	33
	34
	34
	34
	35
Chapter 7
Deployment & Scalability	
7.1 Mobile App Deployment	37
7.2 Backend Deployment (Firebase)	37
7.3 Groq Cloud API (for LLM)	38
7.4 Ollama Local Deployment	38
7.5 Monitoring and Crash Analytics	38
7.6 Scalability Considerations	39
7.7 Deployment Pipelines	39
7.8 Auto-Scaling and Load Testing	39
7.9 Geographical Distribution	40
7.10 Maintenance	40
7.11 Edge Scalability – Wearables and Real-Time Data	40
Chapter 8
Future work	
8.1 Integration with Wearable Devices	42
8.2 Multi-Model AI Ensemble	42
8.3 Expanded Language Support	43
8.4 Personalized AI and Learning	44
8.5 Telemedicine Integration	44
8.6 Clinical Validation and Regulatory Approvals	44
Chapter 9
Conclusion	
9.1 Project Achievements	47
9.2 Significance	47
9.3 Research Methodology and Learning	48
9.4 Future Work	48
○	













 









Chapter 1

INTRODUCTION

 

1.1	Introduction
Medicheck AI is a mobile healthcare application developed to offer proactive and personalized medical assistance by utilizing advanced artificial intelligence (AI) and cloud computing technologies. This project addresses the increasing demand for accessible digital health tools that support individuals in managing their well-being outside traditional clinical environments [1].
The application provides three primary features:
1.	AI-powered emergency guidance through a conversational chatbot
2.	Daily health tracking with intelligent logging and reminder functions
3.	Personalized wellness suggestions based on user health data
4.	symptom assessment via AI chatbot and doctor suggestion
By combining these functionalities within a single mobile platform, Medicheck AI enables users to receive timely medical insights and track vital health indicators in real time. This approach supports early intervention and assists in chronic condition management [1]. It is important to note that the app is not a substitute for professional medical advice but rather a complementary tool that guides users until formal healthcare becomes available.
From a system architecture standpoint, Medicheck AI is built using a cross-platform framework Flutter designed for both Android and iOS. Its backend is cloud-based and hosted on Google Firebase, which manages user authentication, data storage, and serverless function execution. This ensures scalability and secure performance across a growing user base [2]. The mobile interface is user-friendly, supporting English and Urdu, and includes voice interaction and medical document scanning capabilities.
The AI system is composed of two major components:
●	A cloud-based large language model (LLM), accessible via the Groq API, that enables symptom checking and responsive conversational interaction [3]
●	A local AI module powered by the Ollama platform, which processes scanned medical documents directly on the device without transmitting data to external servers [4]
This hybrid setup ensures fast, context-aware responses through the cloud model, while safeguarding user privacy by handling sensitive data locally.
1.2 Healthcare Context
The design of Medicheck AI is inspired by emerging trends in digital healthcare technologies. Studies show that mobile health (mHealth) apps powered by AI can effectively support patients and clinicians in personalized disease prevention and management [1]. Chatbots and symptom checker tools are especially valuable in delivering preliminary medical guidance and triage, regardless of geographic or socio-economic constraints [5].
Despite the benefits, challenges remain particularly regarding data privacy, accuracy of AI-generated responses, and ethical compliance. Medicheck AI directly addresses these concerns by encrypting all user data, including logs, vital signs, and scanned files, and constraining its AI models to generate only safe, evidence-based outputs [1]. These design choices align with international data protection standards and responsible AI usage principles, ensuring user trust while delivering advanced healthcare support.

 
Fig 1: illustrates the complete system architecture and component interactions of Medicheck AI.

















Chapter 2

METHODOLOGY



2.1 Introduction
Medicheck AI’s system architecture is modular and organized into three primary layers: (1) the mobile application frontend, (2) backend cloud services, and (3) AI and machine learning integrations. This layered design ensures a clear separation of concerns, thereby enhancing system maintainability and scalability.
The mobile frontend layer handles all user interactions and on-device operations. The backend layer, hosted on the cloud, is responsible for secure data storage, user authentication, and business logic. Finally, the AI layer is composed of specialized modules that conduct intelligent computations such as natural language understanding and image analysis. Communication between these layers occurs through secure internet protocols—for example, Firebase services are accessed via HTTPS, while local AI interactions happen through Flask server APIs.
The system leverages a cloud-backed architecture to achieve real-time data synchronization and efficient user management. Using serverless functions and external AI APIs, the application can scale compute-heavy operations dynamically, enabling it to serve many users simultaneously without compromising performance [2].
 
              Fig 2.1 System Architecture of Medicheck AI showing modular division

2.2 Mobile Application (Frontend)
The frontend of Medicheck AI is a cross-platform mobile application built with Flutter (Dart). This framework allows the use of a single codebase to support both Android and iOS platforms, ensuring consistency across devices. The app manages all user-facing features through an intuitive graphical interface.
2.2.1 User Interface (UI/UX)
The user interface follows responsive design principles and adapts well across various screen sizes. Accessibility features such as scalable text, high-contrast themes, and voice feedback options are integrated to support users with different abilities. The interface is divided into clearly labeled sections for emergency help, health logs, medication scheduling, and AI-powered chat assistance.
Navigation is facilitated through a bottom tab menu, allowing smooth transitions between modules. Real-time feedback is provided during data synchronization or AI interactions for example, through loading indicators or confirmation messages enhancing the overall user experience [1].
2.2.2 Voice Assistant Integration
One of the key innovations in the frontend is the voice-enabled assistant. By leveraging Flutter’s speech_to_text plugin, the app converts spoken commands into text (STT), and with flutter_tts, it delivers voice responses (TTS). This bilingual interface supports both English and Urdu, making it accessible to a broader population in Pakistan and other Urdu-speaking regions.
A user might say, “Log my blood pressure reading, 120 over 80,” and the app will transcribe, store, and confirm the entry vocally. The recognized command is passed either to a local NLP engine or a cloud-based AI module through the Groq API for interpretation. The response is then spoken aloud to the user, creating a natural and hands-free interaction. Voice-based input has proven to be effective in at-home healthcare applications, increasing accessibility and user engagement [3][4].
 
         Fig 2.2: Voice Interaction Flow — from STT to AI processing and vocal feedback
 
 Fig 2.3: Emergency Use Case app sends voice query to Groq AI and returns urgent recommendation

2.2.3 Document Scanner
The mobile app also includes a medical document scanning feature, enabling users to digitize prescriptions and lab reports using the device’s camera. This functionality is built using Flutter’s image capture and processing libraries. Users can either capture a new image or choose one from the gallery. The app then preprocesses the image by adjusting brightness, applying contrast filters, and cropping as needed.
Once preprocessed, the image is sent to an on-device AI module for optical character recognition (OCR) and analysis. If a local server is available, documents may also be transmitted temporarily for enhanced AI processing. Importantly, documents are not stored in the cloud by default; only processed results such as parsed text or summaries are saved in the user’s account, ensuring privacy and security [1].
 
                        Fig 2.4: Medicheck AI’s Medical Report Scanner interface
Overall, the Medicheck AI mobile application acts as the intelligent client, utilizing device features like the microphone, camera, and touch interface to gather user data. It supports some offline functions, such as viewing logs or using basic voice commands. However, full functionality is achieved when the app is connected to the internet, enabling real-time AI processing and data sync.
The decision to adopt Flutter was influenced by its strong performance, wide developer adoption, and ability to deliver native-like responsiveness across platforms. These qualities make Flutter particularly suitable for healthcare app development, where reliability and cross-device consistency are critical [2]. 
2.3 Backend Services (Firebase Cloud Backend)
The backbone of Medicheck AI’s infrastructure is built on Google Firebase, which delivers a reliable and scalable backend-as-a-service (BaaS). This backend layer manages user authentication, data storage, business logic, and integration with external services. The decision to use Firebase was driven by its real-time capabilities, robust security, and built-in scalability key requirements for a health application handling sensitive medical data [1].
2.3.1 Authentication
Medicheck AI employs Firebase Authentication for secure user registration and login. Users can sign up with an email and password or authenticate using their Google account. The system includes protections such as email verification, strong password enforcement, and built-in safeguards against common attack vectors.
Each user’s data is isolated using Firebase security rules. Access permissions are granted only to the database nodes tied to that user's unique ID. This role-based access control ensures no user can view another’s health data. By leveraging Firebase’s authentication framework, Medicheck AI adheres to best practices commonly required for healthcare applications, including support for multi-factor authentication and audit compliance aligned with standards such as HIPAA.
2.3.2 Cloud Firestore Database
All persistent user data is stored in Cloud Firestore, Firebase’s NoSQL cloud-hosted database. The data model is organized into collections such as Users, HealthLogs, Medications, and ChatLogs. Each user has a profile document containing demographic data, chronic conditions, and sub-collections for logs, medications, and medical documents.
Firestore supports real-time synchronization and offline caching, allowing users to record data without internet access. Changes sync automatically once the network is available. The system also applies end-to-end encryption, protecting data at rest and in transit. Additional client-side encryption is used for sensitive fields, such as user notes or free-text entries [2].
This architecture enables features like live medication schedule updates. If a user or physician adjusts medication data, changes reflect immediately in the app ensuring timely and accurate notifications.
2.3.3 Firebase Cloud Functions
Firebase Cloud Functions are used for running secure backend logic in response to real-time triggers. These serverless functions scale automatically and eliminate the need for managing infrastructure.
●	AI Request Proxy: When a user interacts with the chatbot, a cloud function intermediates the request, securely passing it to the Groq AI API. This shields API keys from the client, allows post-processing (e.g., filtering or formatting responses), and centralizes logic if the AI service provider changes [3].
●	Notification Scheduler: This function monitors the medication database and interfaces with Firebase Cloud Messaging (FCM) to schedule and send reminder notifications. For instance, if a dose is scheduled at 8:00 AM, the function generates and delivers a push alert even if the app is inactive.
●	Data Analytics and Processing: Scheduled functions (cron jobs) perform backend analytics such as generating health summaries or cleaning outdated logs. These functions can also send weekly reports or alerts based on usage trends.
By offloading compute-intensive and sensitive logic to these functions, Medicheck AI maintains a secure, scalable backend while keeping the app lightweight.
2.3.4 Firebase Cloud Messaging (FCM)
FCM handles all push notifications, such as medicine reminders or urgent alerts. It supports background notifications that direct users to specific app screens when tapped. For example, a reminder alert can take the user straight to the Medication tab. FCM ensures timely, targeted engagement even if the app is not actively running [4].
2.3.5 Backend Compliance and Reliability
Firebase offers industry-grade compliance, including ISO/IEC 27001, GDPR, and HIPAA-readiness (when configured with data use agreements). The same stack is used by healthcare platforms such as Jevitty, validating Firebase’s capability to handle medical data securely [5].
Overall, the Firebase backend enables real-time sync, secure access control, cloud-triggered automation, and scalable integration with third-party AI APIs—all without managing physical servers.
2.4 AI & Machine Learning Integrations
A key innovation of Medicheck AI lies in its integration of state-of-the-art AI and machine learning (ML) services to deliver intelligent healthcare features. Instead of training custom models from scratch, the system integrates reliable, production-ready models into modular services.
Table 1.Features and Integrated Models in Medicheck AI

Feature
AI Model/Service	Purpose
Symptom Checker & Chatbot	Groq Cloud API using Llama-4 Maverick 17B (Meta)	Medical Q&A; symptom analysis and triage
Medical Report Analysis	Ollama Local Server running Llama-3.2-Vision	OCR and summarization of scanned documents
Voice Assistant	Flutter packages (speech_to_text, flutter_tts)	Bilingual voice interaction (English/Urdu)
Doctor Suggestion 	Json format 	suggest the local doctor based on the symptoms 


2.4.1 Symptom Checker
This feature utilizes a large language model (LLM) hosted via the Groq API, enabling real-time medical Q&A and basic triage support. The model interprets natural language queries from users and generates reliable, evidence-based advice. The LLM is only invoked during chatbot interactions or when AI analysis of a voice input is necessary [3].
 
                                             Fig 2.5:Workflow Symptom Checker

2.4.2 Medical Report Analysis
Scanned documents (like prescriptions and lab results) are processed using a vision-capable AI model hosted on a local server via Ollama. This ensures that private documents remain on the user’s device or local network during processing. OCR and summarization modules extract and structure key data without uploading it to third-party servers, enhancing privacy [2].
2.4.3 Voice Assistant
Medicheck AI’s bilingual voice interface uses proven libraries (speech_to_text, flutter_tts) for converting spoken language to text and vice versa. Though these tools don’t perform reasoning or deep inference, they facilitate hands-free interaction, improving accessibility and user engagement.
Each AI module is used only when needed, reducing resource consumption and maintaining efficiency. This modular approach makes it easier to upgrade components over time as better models become available without major changes to the app or backend.
 
                                                 Fig 2.6:Voice Assistant Working
2.5 Voice Assistant (Flutter Implementation)
2.5.1 Introduction:
Voice interaction has become a vital component in modern health applications, providing accessibility and convenience, particularly for users who experience difficulty with typing or require hands-free operation [1]. In Medicheck AI, the Voice Assistant is implemented fully on the client side within the Flutter framework. It allows users to interact using speech, which is converted into text, processed by the app or AI backend, and then spoken back to the user. This chapter outlines the core technologies, workflow, user benefits, and key challenges of the voice assistant implementation.
2.5.2 Technologies Used
The voice assistant functionality is built using two Flutter plugins: speech_to_text and flutter_tts. The speech_to_text plugin manages speech recognition by accessing the device’s microphone and transcribing audio to text. On Android devices, it integrates with Google’s Speech API, while on iOS it uses Apple’s native speech framework. This component supports both English and Urdu, with configuration relying on system-level language models when available.
The second plugin, flutter_tts, is used to convert output text into speech. It utilizes the built-in text-to-speech engines on the device to deliver feedback in audio form. Like the recognition plugin, it supports multiple languages and voices, which enables the app to provide feedback in both English and Urdu based on user preference. These plugins abstract the underlying complexities of speech signal processing and allow the development team to integrate voice features efficiently. To comply with privacy and permission policies, the app requests microphone access the first time a user attempts to use the voice assistant.
2.5.3 Voice Interaction Workflow
A typical interaction with the voice assistant begins when the user activates the voice input by tapping a microphone icon. While future enhancements may include hotword detection such as “Hey Medicheck,” the current system opts for manual activation to avoid the performance and privacy trade-offs associated with continuous listening.
Once activated, the app prompts the speech_to_text plugin to begin listening, and a visual indicator such as a pulsing microphone icon confirms recording status. The user then issues a command or query, which may range from simple actions like “Log my blood pressure” to more complex queries such as “What should I eat to help with diabetes?” For Urdu-speaking users, commands in Urdu are also supported, provided the OS speech model includes it.
The plugin transcribes the user's speech, displays the final recognized text on-screen for verification, and triggers the appropriate logic. If the spoken input corresponds to a direct action, like logging health data, the app processes it locally. In contrast, if the query requires medical reasoning such as treatment questions or medication conflicts—the transcribed input is sent to the Groq API through a cloud function, and the response is returned as text. This text is then spoken back to the user using flutter_tts, while also being displayed on-screen for reference. Each interaction is treated as a one-turn exchange, though the chatbot mode supports multi-turn queries with manual reactivation of listening.
 
Fig 2.7: App Workflow in Medicheck AI from user input to voice interaction and output
2.5.4 Example Use Case
Consider a user busy in the kitchen who wants to log their medication and ask a health-related question without stopping their activity. The user says, “Log that I took 1 tablet of metformin 500 mg now.” The voice assistant transcribes the statement, stores it in Firestore, and responds with a confirmation: “Recorded: 1 tablet of Metformin 500 mg at 2:30 PM.” Next, the user asks, “What should I eat to help with diabetes?” The query is sent to the AI backend. The response is delivered both as voice and text: “A balanced diet low in simple sugars and rich in fiber can help manage diabetes. Focus on vegetables, whole grains, and lean protein. Avoid sugary drinks. Always consult your doctor for personalized guidance.” [2]
This interaction illustrates the hands-free utility of the voice assistant and its ability to seamlessly manage logging and AI-powered responses.
2.5.5 Technical Challenges and Solutions
Several implementation challenges were encountered:
●	Medical Term Recognition: Speech engines struggled with complex drug and condition names. We resolved this by providing custom vocabulary hints (where supported), including common medications and medical terms in English and Roman Urdu.
●	Urdu Language Support: Recognition accuracy for Urdu was lower. Users were encouraged to use English medical terms where needed. We anticipate improved Urdu support with newer STT engines.
●	TTS Optimization: To enhance clarity, we chose clear and neutral voices a female voice for English and a male voice for Urdu. Long responses were broken into shorter phrases with pauses to improve speech naturalness.
2.5.6 Privacy Considerations
All voice data is processed on-device. Audio is neither stored nor transmitted to any server. Only the transcribed text is sent to the backend (when required). The assistant activates listening only upon explicit user action, and immediately stops afterward. This privacy-first design ensures that sensitive voice data remains local, aligning with healthcare compliance practices.
2.6 Medicine Reminder System
2.6.1 Importance of Medication Adherence
Medication adherence is a critical aspect of chronic disease management, and missing doses can have serious health consequences. To support users in maintaining consistent medication routines, Medicheck AI includes a dedicated Medicine Reminder System. This system enables users to schedule medications and receive push notifications at precise times, reducing the likelihood of missed doses. Numerous studies have demonstrated that mobile health reminders significantly improve patient adherence, particularly in chronic care scenarios where treatment regimens are long-term and complex [1].
2.6.2 Setting Up Medications
In the Medicheck AI interface, users can access the medication section to add and manage their prescriptions. Each entry includes the medication name, dosage instructions, timing and frequency, start and end dates, and optional notes. For example, users can specify, “1 tablet of Metformin 500mg twice daily at 8:00 AM and 8:00 PM,” along with an instruction such as “Take with food.” The input process is wizard-style, guiding users through simple steps to avoid the need for complex scheduling syntax. Common medication templates are also available, allowing for quick setup of frequently prescribed courses, such as antibiotics taken three times daily for seven days.
 
                 Fig 2.8: Workflow of the Medicine Reminder System in Medicheck 
2.6.3 Data Storage in Firestore
Medication schedules are stored securely in Firestore under the user's individual Medications collection. The data structure supports querying upcoming doses by capturing timing, frequency, and note fields in a searchable format. This backend architecture ensures that each user's medication data is isolated and protected, while remaining accessible for the app's scheduling and analytics components.
2.6.4 Notification Mechanism
The notification system is powered by Firebase Cloud Messaging (FCM) and Firebase Cloud Functions. A scheduled background function, known as scheduleNotifier, checks the database at fixed intervals typically every hour to determine if any users have doses scheduled in the upcoming window. It also accounts for immediate triggers when a new dose is due shortly after being added.
Each notification is composed with context from the user’s medication entry. For example, the message might read, “Time to take your Metformin 500mg – 1 tablet now.” If notes are present, they are included for clarity: “(Remember: take with food.)” The notification is sent directly to the user’s device using a unique FCM token, and it appears even if the app is closed. Tapping the alert opens the app to a confirmation screen, where the user can either confirm the dose as taken or choose to snooze it.
2.6.5 Customization and Flexibility
Users have full control over their medication schedules. They can edit or cancel reminders, pause a medication, or adjust timing and dosage instructions as needed. In scenarios where internet connectivity is unavailable, the app falls back to local device alarms to deliver basic alerts. Although these local notifications lack cloud precision, they ensure continuity in reminding users to take their medicine.
The app also includes a summary dashboard showing all upcoming doses for the day. After a reminder is triggered, the user’s response whether they took or skipped the dose is recorded in Firestore with a timestamp. This data helps users track adherence and can also be useful for physicians monitoring treatment effectiveness. The app later provides insights like, “You have taken 90% of your doses on time this month,” offering motivational reinforcement.
2.6.6 Effectiveness and Clinical Relevance
The reminder system effectively addresses one of the most common causes of poor outcomes forgetfulness. For users managing chronic diseases such as diabetes or hypertension, this feature reduces cognitive burden and enhances treatment consistency. Additionally, since the reminder system integrates with the AI assistant, users can respond to a notification with a follow-up question, such as, “I feel dizzy after this pill, is that normal?” initiating intelligent support when needed [1].
2.6.7 Security and Privacy Considerations
Since medication data is inherently sensitive, Medicheck AI takes careful measures to protect user privacy. All schedules and logs are encrypted both in transit and at rest. Recognizing that notifications like “Take your Metformin” may inadvertently disclose health status, the app includes a discreet mode. When enabled, this mode replaces specific medication names in the notification with generic phrases such as, “Time to take your medication,” requiring the user to open the app to see the details. This feature allows users to choose between privacy and convenience.
2.6.8 Planned Enhancements
Future updates will expand the system’s utility by integrating with external services. For example, users may be able to sync medication schedules with Google Calendar or receive email summaries for caregivers. Though not implemented in the current version, these enhancements are part of the planned roadmap to make Medicheck AI even more comprehensive in supporting chronic care.
2.6.9 Real-World Example
A patient with hypertension sets a daily 7:00 AM reminder for Amlodipine 5mg. At the scheduled time, the app sends a gentle notification: “Medicheck Reminder: Take 1 pill of Amlodipine 5mg now (Hypertension med).” The user taps “Mark as Taken” to log the action. If they skip the dose, the app may follow up the next day with a query: “You missed your Amlodipine yesterday. Would you like to reschedule or stop this medication?” This interactive approach reinforces adherence and helps form consistent habits.
2.7 Software Engineering Model Used
Agile methodology was employed in the development of the Medicheck AI application. This approach is a widely adopted software development framework characterized by iterative progress, adaptability, and continuous improvement. Agile emphasizes flexibility and close collaboration among cross-functional team members. Projects are divided into small, manageable increments called sprints, and after each sprint the team evaluates progress and integrates feedback. This cyclical process ensures that development can quickly adapt to changing requirements or user needs, maintaining alignment with project objectives.
Key principles of Agile include iterative development, stakeholder collaboration, and continuous feedback loops. Rather than following a rigid long-term plan, Agile focuses on delivering functional software in short cycles and embraces changes as new information and feedback emerge. Frequent reassessment of work and open communication with stakeholders are integral to this methodology, ensuring the project remains responsive to necessary modifications. These principles foster a culture of constant refinement and quality improvement throughout the development lifecycle.
In the context of the Medicheck AI app, adopting an Agile approach was particularly beneficial. The development team implemented features through successive sprints, which allowed for incremental progress and regular checkpoints to evaluate outcomes. Early involvement of end-users provided timely feedback that guided immediate adjustments and enhancements to the system. Through these real-time testing and feedback cycles, the product was continuously refined to better meet user requirements and to address any issues promptly. This iterative strategy not only accelerated the development process by identifying and resolving potential problems early, but also improved the application's reliability and user satisfaction. The agility afforded by this methodology ensured that Medicheck AI evolved effectively in response to real-world testing and stakeholder input, resulting in a more robust and user-aligned final product.
2.8 Security & Compliance
Handling medical data and providing health advice impose stringent requirements on security and regulatory compliance. Medicheck AI has been designed with privacy and data protection as core principles, in order to comply with healthcare standards such as the Health Insurance Portability and Accountability.
2.8.1 Data Encryption
All user data transmitted and stored by Medicheck AI is encrypted. Communication between the app and Firebase backend uses HTTPS with TLS encryption, preventing eavesdropping on the network. In the Firebase Firestore database, data is encrypted at rest on Google’s servers. For an extra layer, especially sensitive fields (for instance, personal identifiers or specific health metrics) are client-side encrypted using a key derived from the user’s login credentials or a device key. This means that even in the unlikely event of unauthorized access to the database, the data would be gibberish without the decryption key. End-to-end encryption of health logs and documents is thus ensured, aligning with recommendations for protecting health information [1]. We also use Firebase’s security rules to enforce that data can only be read/written by authorized requests and, where applicable, only by the owning user (as described in Chapter 2.2).
For the document images that are analyzed, if any are temporarily stored (on the local server running Ollama), those are kept in an encrypted filesystem, and they are deleted immediately after processing. No scanned image is stored long-term unless the user saves it explicitly (which in our current design, they do not – only the extracted text or summary is saved). This reduces the risk of sensitive images being exposed.
2.8.2 User Authentication & Access Control
As detailed earlier, we rely on Firebase Authentication, which provides robust measures such as email verification and optional two-factor authentication. Passwords are never stored in plaintext; Firebase handles hashing and verification, following industry standards (bcrypt or scrypt hashing with salts). Session tokens are securely managed. Furthermore, we implement Role-Based Access Control (RBAC) at a basic level: there are two roles – user and admin. Normal users can only access their own data. There is an admin console (for project maintainers or for a health provider in a deployed scenario) that could allow viewing aggregated anonymized data or managing the model settings, but no admin can view personal user logs unless explicit consent is given and audit trails are recorded.
Within the app, sensitive sections (like viewing a PDF of a medical report, or exporting data) can be protected by an app-specific PIN or biometric unlock, so that if someone else is using the phone they cannot casually browse the Medicheck data. This feature is in line with privacy best practices for mobile health apps, offering a second layer of protection in case the phone itself is unlocked by someone else.
2.8.3 Ethical Handling of Medical Data
Medicheck AI prioritizes the ethical and secure handling of health-related data. All user information is encrypted, protected by access control, and never shared without user consent. We ensure transparency through user agreements and offer control features such as data deletion and export options. While no specific international laws are enforced, the system is designed around global privacy best practices, with respect for patient trust and digital safety.
2.8.4 Consent and User Control
When a user first uses Medicheck AI, especially the AI features, we present a disclaimer that this is not a doctor and the advice is for informational purposes. This is to manage liability and also to be transparent. Users must agree (via a checkbox) that they understand this. For data usage, the sign-up flow includes consenting to data handling as per our privacy policy. If we ever integrate research features or share anonymized data for studies, we would add a specific consent for that (but currently, we do not share data with any third parties). Users can also opt out of certain data collection – for example, we allow them to toggle off crash reporting or analytics, which by default we have via Firebase Crashlytics and Analytics in Chapter 7.
2.8.5 Security Testing
We performed basic penetration testing on the app. This included trying to bypass authentication (which was thwarted by Firebase rules), testing the encryption by attempting to read the database directly (could not without proper credentials), and ensuring that all external requests (to Groq API and others) are done over secure channels. We also ensure that no hard-coded secrets are in the app bundle – API keys for external services are stored in cloud function environment variables, not in the app code, to prevent extraction.
Additionally, the app’s code obfuscation was enabled in release builds to protect against reverse engineering. While this is not foolproof, it raises the bar against malicious actors trying to tamper with the app or understand its inner workings.
2.8.6 Continuous Compliance
The regulatory landscape may change, so we designed the system to be adaptable. For instance, if a new law requires data localization (storing data in the user’s country), we could migrate the backend to servers in that region (Firebase allows choosing data centers). Our architecture centralizes most data in one platform which simplifies compliance updates.

 

                                  Fig 2.9: Medicheck AI Application Screenshots















Chapter 3

AI MODELS & PROMPT ENGINEERING













This chapter outlines the two core AI components integrated into Medicheck AI: the Groq API-based symptom checker and the Ollama-based local medical report analyzer. For each module, the underlying AI model, prompt engineering strategies, and safety measures are detailed. The emphasis is on adapting advanced AI capabilities to meet strict requirements of the medical domain, prioritizing reliability, accuracy, and user safety.
3.1 Groq API – Symptom Checker & Chatbot
Medicheck AI utilizes the Groq API to access a powerful large language model (LLM), used as the app’s symptom checker and general medical chatbot. Specifically, the deployed model is Meta’s Llama-4 Maverick (17B parameters, 128 experts), a cutting-edge LLM known for high performance in multi-language and instruction-following tasks [1]. Meta’s Llama models have demonstrated expert-level proficiency in medical exams and reasoning tasks [2].
 
Fig 3.1: AI Query Processing Pipeline in Medicheck AI including prompt construction
3.1.1 Model Deployment & Performance
The LLM is hosted on Groq’s proprietary LPU (Language Processing Unit) systems, enabling ultra-fast inference speeds. In testing, initial responses were typically returned within one second, even for complex queries. This low-latency design is essential for use cases like emergency health guidance, where timely information is critical [3].
3.1.2 Prompt Engineering for Medical Use
To ensure accurate and responsible outputs, each AI interaction begins with a system prompt, configured at the cloud function level. The model is instructed to behave as a medical assistant, providing safe, concise, evidence-based responses. A simplified example of the prompt is:
You are a medical AI assistant. Provide concise, evidence-based advice for the user’s symptoms. If serious or emergency symptoms are detected, advise immediate medical attention. Otherwise, offer possible causes and safe remedies. Maintain a professional, empathetic tone.
3.1.3 Example Query Interaction
User Input: “I have a headache and slight fever since this morning, should I be worried?”
 AI Output:
 "It sounds like you may have a mild viral infection such as the flu. Monitor your temperature, rest, and hydrate. Acetaminophen can help with symptoms. Seek medical help if your fever exceeds 39°C, or if symptoms worsen (e.g., severe headache or stiff neck)" [2].
All responses are checked by cloud functions for content length, prohibited terms, and response formatting before being returned to the app.
3.1.4 Domain Knowledge Integration
To enhance safety, domain-specific validation is implemented. For instance, when users mention medications, the system references an embedded knowledge base to verify dosages or chemical names.
Example:
 User asks, “I took 500 mg of acetaminophen, can I take ibuprofen too?”
 System recognizes acetaminophen as paracetamol (C₈H₉NO₂) and cross-references usage compatibility before responding [4].
This combination of LLM + domain data minimizes hallucination risks and improves clinical reliability.
3.1.5 Safety Biasing and Response Strategy
The model is instructed to:
●	Keep answers short and focused
●	Avoid speculation or casual tone
●	Default to cautious recommendations when symptoms suggest any health risk [2]
This may occasionally result in over-caution (“over-triage”), but it aligns with the app’s goal to minimize harm and misdiagnosis risks.
The Groq-based symptom checker successfully transforms the app into an intelligent, multilingual health companion providing real-time guidance with built-in medical reasoning and user-centric safeguards [1][2][3].
3.2 Ollama – Medical Report Analysis (Local Vision Model)
The second major AI feature of Medicheck AI is its Medical Report Analysis module, which interprets scanned documents such as lab reports or prescriptions. This is powered by the Ollama platform and Meta’s Llama-3.2-Vision, a multimodal model capable of understanding both images and text [5].
3.2.1 OCR and Data Extraction
When a user scans a report via the app, the image is sent to a local server running Llama-3.2-Vision (11B). The model performs OCR (Optical Character Recognition) directly on the image using its pretrained image-text capabilities. For highly structured formats (like lab tables), results are enhanced using Tesseract OCR, and the text is passed to the model for structured summarization [6].
3.2.2 Summarization and Medical Interpretation
A custom prompt guides the model to:
●	Extract key test names and values
●	Identify abnormal results
●	Explain findings in plain language
Example output:
●	Hemoglobin: 10.2 g/dL (Low) – May indicate anemia
●	WBC Count: 12,000/µL (High) – Suggests potential infection
●	LDL: 180 mg/dL (High) – Risk factor for cardiovascular disease
The model is also prompted to provide a brief summary and recommended follow-up actions.
3.2.3 Safety Rules and Validation
To mitigate AI misinterpretation, a ruleset checks output against medical thresholds. For example:
●	Hemoglobin < 8 g/dL → Flag as critical anemia
●	Temperature > 40°C → Mark as emergency
This hybrid AI-rule approach ensures model outputs remain medically sound.
3.2.4 Local Deployment Benefits
Running the model locally provides:
●	Data privacy: Documents never leave the user’s network
●	Offline functionality: App can analyze reports even without cloud access
●	Security compliance: Sensitive health data is not exposed to third-party APIs [6]
The system was tested on a mid-range GPU (8GB VRAM), hosted on a local machine connected over Wi-Fi. Results were processed efficiently in under 15 seconds per report.
3.2.5 Integration and Display
After processing, summaries are sent to Firebase Firestore, attached to the user’s profile. The app displays these under a “Report Analysis” section. Users can tap any item to see:
●	Normal ranges
●	Explanation
●	Recommendations
Raw extracted text is also stored as a hidden field for audit or physician review.
The Llama-3.2-Vision model has shown impressive performance in multimodal benchmarks and has proven effective in interpreting real medical data. By combining this capability with controlled prompts and a validation layer, Medicheck AI turns dense medical reports into actionable insights boosting user health literacy and empowering proactive care [5][6]

















                                          Chapter 4

                           DEPLOYMENT & SCALABILITY









For Medicheck AI to be useful in real-world scenarios, it must be deployable on modern infrastructure and scale to accommodate potentially large numbers of users or high loads (especially during health crises when many might use the service simultaneously). This chapter discusses how the application is deployed in its current form and how it scales, as well as tools used for monitoring and maintaining the system.
4.1 Mobile App Deployment
The Flutter mobile application is packaged for both Android (APK/AAB) and iOS (IPA) distribution. It can be deployed via app stores or sideloaded for testing. Flutter’s framework ensures that the release builds are optimized – including tree shaking to remove unused code, which keeps the app size reasonable despite the many packages. The app doesn’t hardcode any server URLs except the Firebase endpoints, which are configured via GoogleServices-Info.plist and google-services.json (downloaded from our Firebase project). This means we can easily point the app to a different backend by switching those config files if needed (for example, a separate production and development environment).
4.2 Backend Deployment (Firebase)
One advantage of using Firebase is the reduction in DevOps overhead. Firebase Authentication, Firestore, and FCM are fully managed by Google, so there is no traditional server to deploy or manage for those components. We did, however, create Cloud Functions (Node.js runtime). These functions were deployed using Firebase CLI tools; once deployed, Google auto-scales them. For instance, if a burst of 1000 chatbot queries comes in at once, Firebase will spin up multiple instances of our function to handle them in parallel, then scale down when done. This serverless model inherently handles scalability for the backend logic.
We chose a Firebase Blaze Plan which supports scaling with usage (pay-as-you-go). It imposes some concurrency limits (e.g., 1000 concurrent function invocations by default), but those can be raised. In our tests, the architecture easily handled simulated loads of a few hundred simultaneous users performing actions, with no performance degradation – the Firestore and functions kept response times in the sub-second range, owing to Firebase’s global infrastructure.

4.3 Groq Cloud API (for LLM)
The Groq-based LLM is a cloud service provided by Groq Inc. We accessed it via their API endpoint. This service itself is built for scale – Groq Cloud runs on clusters of Groq LPU hardware that can handle many requests in parallel. From our perspective, scaling here means potentially hitting rate limits if our user base grows. Currently, the usage is moderate, but if needed, we can obtain a higher-throughput plan or even host an instance of the model on our own (Groq offers on-prem deployment for enterprises, though that is beyond our project’s scope). The latency of Groq’s API was consistently low (a few hundred milliseconds to first token), which means even as we scale to more users, each user’s experience of quick responses should remain – provided the cloud capacity scales, which we trust Groq to manage or we contract accordingly.
4.4 Ollama Local Deployment
The Ollama service for the vision model is the one component not inherently cloud-scalable out of the box. In the current setup, it runs on a single server. To scale this for more users or heavy use, we have a few strategies. For an organization (like a clinic) using Medicheck AI, they could run Ollama on a local machine that all user apps in that network communicate with. If many users might send documents at once, we could deploy multiple such servers and use a simple load-balancer that the app contacts (perhaps via the cloud function which then routes to one of the available analysis servers). This manual scaling is feasible since document analysis requests are likely less frequent than chat queries.
We anticipate that in the future, we could replace or complement the local model with a cloud vision model (for instance, if Groq or others offer Llama-3.2-Vision via API). That would allow horizontal scaling similar to the chatbot. Until then, we treat the document analysis as a specialized service possibly provisioned per deployment environment.
4.5 Monitoring and Crash Analytics
We integrated Firebase Crashlytics for crash reporting in the app. This tool collects any runtime errors or crashes from the app and aggregates them on a dashboard. It helps identify issues at scale e.g., if a particular device model has a problem or a certain sequence of actions triggers a bug, we get reports and stack traces. During development, Crashlytics already helped catch a few edge cases (like a null pointer when voice recognition returned an empty result). In deployment, it will be crucial for maintaining app stability across thousands of different phone models and OS versions.
We also use Google Analytics for Firebase to monitor user engagement (with the user’s consent). This gives metrics on active users, the frequency of feature usage (e.g., how many use the voice assistant, how often the AI chatbot is invoked), and user retention. While not directly related to “scalability,” these analytics inform us if the system is coping well or if certain flows cause user drop-off, which might indirectly signal performance issues.
4.6 Scalability Considerations
One area to consider is data scaling. As users accumulate logs and documents, Firestore will grow. Firestore is very scalable and can handle large volumes, but we have set up some measures like data retention policies (for example, maybe delete chat logs older than a year, unless flagged to keep) to prevent unbounded growth. We also ensure queries are indexed properly to keep read latency constant even as data size grows (Firestore requires indices for certain composite queries, which we added).
4.7 Deployment Pipelines
For maintainability, we set up CI/CD pipelines (using GitHub Actions) that automatically test and deploy the Cloud Functions when code is updated. The mobile app build is also automated to run tests and produce build artifacts, though releasing to app stores is still a manual step (with potential to automate via Fastlane in future). This ensures that updates (especially to the backend) can be rolled out quickly and reliably, which is essential if a bug or security issue needs immediate fixing.
4.8 Auto-Scaling and Load Testing
We conducted load tests for the chatbot by simulating 100 concurrent users asking questions continuously over a short period. The Groq API and our cloud function handled it with average response times around 1–2 seconds, which is acceptable. Firestore and Auth are well behind Google’s CDN and can handle tens of thousands of concurrent users easily by design. The limitation was more on cost (API usage cost, etc.) than technical. We set up budgeting alerts on Firebase to monitor if usage (and thus cost) spikes beyond expected, so we can take action (optimize code or upgrade plan).
4.9 Geographical Distribution
Firebase has the option to choose regions for data storage. Currently, our Firestore is located in a single region (USA) for simplicity. As the user base scales internationally, we may leverage multi-region replication or deploy separate instances in different continents to reduce latency. For now, the app being a student project, one region suffices, but the groundwork to migrate to multi-region is straightforward with Firebase (export and import data).
4.10 Maintenance
The system requires minimal maintenance thanks to managed services. However, we do plan routine tasks like updating the AI models (for example, if a new version of Llama or a medical-specific model with better performance becomes available, we would update the integration). The modular architecture makes this feasible – e.g., switching the model used by Groq API if needed, or updating the local model weights.
4.11 Edge Scalability – Wearables and Real-Time Data
In future enhancements (Chapter 8), we consider integrating wearable data streams (continuous heart rate, etc.). This could increase data influx significantly (many data points per minute). We mention this to acknowledge that our current design (Firestore writes for each log) might need adaptation – perhaps moving to time-series databases or batch processing. But this is a future concern; the current system is well-suited for the relatively low-frequency events it handles (user queries, daily logs, .






















                                            Chapter 5


                                        Conclusion










Medicheck AI represents the fusion of mobile technology and artificial intelligence to create a proactive personal health assistant. In this final chapter, we summarize the achievements of the project, reflect on its significance in the context of digital health, and consider the path forward for its deployment and impact.
5.1 Project Achievements
Through the course of this project, we successfully developed a cross-platform mobile application that integrates a range of advanced features: AI-driven medical chatbot, voice-assisted interaction, automated health logging, medication reminders, and document analysis. The system architecture leveraged Firebase for a secure and scalable backend, and cutting-edge AI models (Meta’s Llama family via Groq, and Llama Vision via Ollama) for providing intelligent functionalities. We implemented robust security measures, including encryption and access controls, to protect user data – an essential requirement for any health-related application [1]. The final system was tested in scenarios simulating real use-cases and performed well, providing timely responses and demonstrating the potential to improve user engagement in health management (for instance, simplifying complex information and giving actionable suggestions). All these were accomplished with a user-friendly interface that abstracts the complexity of the underlying technology, proving that sophisticated AI can be delivered in an accessible manner on everyday devices.
5.2 Significance
Medicheck AI addresses a growing need in healthcare: empowering individuals with tools to manage their health and make informed decisions daily, not just during doctor visits. By providing instant emergency guidance and general health advice, the app can fill gaps especially in settings where medical access is limited or delayed. The integration of multilingual support and localization for Pakistan’s context (Urdu language, local health nuances) makes it culturally relevant. Moreover, by logging health metrics and adherence, the app encourages a more disciplined approach to health – for example, reminders ensure medications are taken on time, which is known to improve outcomes in chronic diseases [1]. The AI components, while not a replacement for professional care, serve as an augmented layer of support, offering evidence-based suggestions and information that can alleviate anxiety and improve knowledge for users. This aligns with the broader trend of AI in mHealth to assist in chronic disease management and preventive care [1]. Importantly, our approach kept human oversight in the loop: we emphasize in the app that doctors are the ultimate authority and provide means to connect users with healthcare providers when needed.
5.3 Research Methodology and Learning
This project followed a comprehensive development and research methodology, including requirement analysis (identifying user needs for emergency help, monitoring, etc.), system design (chapter-wise architecture planning), implementation, and evaluation (testing features and ensuring compliance with standards). We consulted scientific literature and best practices at each step – from using known secure architectures in health apps [11] to implementing AI models reported to be effective in medical contexts [2]. This not only strengthened the credibility of our design choices but also grounded the project in the current state of the art. Throughout, we maintained an academic rigor in documentation, as seen by the inclusion of real references, which could allow others to trace the knowledge and techniques we built upon.
5.4 Future Work
As discussed in Chapter 8, there are numerous avenues to extend Medicheck AI. The project, in many ways, is a proof of concept demonstrating how various technologies can come together to create an intelligent health assistant. Moving forward, the focus will be on refining the AI’s medical knowledge (perhaps via fine-tuning on medical datasets), formally validating the system’s recommendations with healthcare professionals (to measure accuracy and safety), and scaling up the user testing. We also plan to incorporate user feedback to improve usability – for example, simplifying the onboarding process or adding features users specifically request. Another important step is regulatory alignment if we aim for real-world deployment: engaging with medical device regulators or ethics boards to ensure our app meets all necessary guidelines for public use.























                                       Chapter 6

                                    FUTURE WORK










While Medicheck AI in its current state provides a comprehensive set of features, there are several opportunities to enhance and expand its capabilities. In this chapter, we outline some key future enhancements that have been identified. These include integrations with wearable devices, improvements in AI accuracy through ensemble modeling, and broader language support to cater to a more diverse user base. These enhancements would further align the project with the evolving landscape of digital health and user expectations.

6.1 Integration with Wearable Devices
One natural extension is to integrate data from wearable health trackers (such as fitness bands, smartwatches, or IoT health devices). Wearables can provide continuous streams of health metrics like heart rate, physical activity (step count, sleep patterns), blood oxygen levels, or even ECG readings in advanced devices. By connecting Medicheck AI with platforms like Google Fit, Apple HealthKit, or specific APIs from devices (Fitbit, Garmin, etc.), the app could automatically import these data points. This would enrich the user’s health profile without manual entry and enable the AI to give more personalized insights. For example, if a user’s wearable reports sedentary behavior and poor sleep over a week, Medicheck AI’s chatbot could proactively suggest tips for improving sleep hygiene or ask if the user is feeling stressed. Integration with wearables also means the app could potentially detect certain anomalies (like an arrhythmia from an ECG or an unusually high resting heart rate) and alert the user promptly – moving toward a preventative care assistant. Technically, implementing this would involve using each wearable’s SDK or a unified service (some exist to aggregate data). Privacy considerations are paramount; user consent must be obtained to fetch this data, and the data should be treated with the same level of security as manually entered data. This feature aligns with the trend in mHealth to provide a holistic view of patient health by combining self-reported and device-reported data [1].
6.2 Multi-Model AI Ensemble
Currently, Medicheck AI uses single models for each major task (one LLM for chatbot, one vision model for OCR/analysis). An improvement could be to adopt a multi-model ensemble approach for critical tasks to improve accuracy and reliability. For instance, for symptom checking, we could incorporate a second opinion from another model (like GPT-4 or a medically fine-tuned smaller model) and then combine responses – for example, only provide the answer if both models agree, or use the second model to verify the first model’s suggestion. Ensemble learning is known to enhance performance by leveraging the strengths of different models and averaging out their errors. In the context of medical advice, this could reduce the chance of an incorrect suggestion going through unchecked. Similarly, for image analysis, an ensemble of an OCR specialist model and a medical NLP model could improve extraction and interpretation accuracy. We could also integrate traditional rule-based expert systems for certain domains as a check – for example, a small knowledge-based system for drug interactions that runs in parallel when the AI suggests something, flagging any unsafe combination the AI might have overlooked. Implementing ensembles would require more computation and careful design of how to reconcile outputs, but it would push Medicheck AI’s reliability closer to that expected of clinical decision support systems. Additionally, as new specialized models emerge (say, a model specifically trained on medical Q&A or a model for dermatology image analysis), they can be added to the system as additional modules that the app can route queries to. This can be seen as an ensemble or a specialization selection.
6.3 Expanded Language Support
Currently, the app supports English and Urdu. Pakistan and many regions have linguistic diversity, and globally, a health app can reach more people by supporting more languages. We plan to extend support to other languages such as Arabic, Hindi, and Spanish, among others. This involves two aspects: the user interface localization, which entails translating all app menus, prompts, and messages into the target languages. Flutter’s localization tools can aid in this, and we would work with medical translators to ensure the terminology is correct (for instance, medical terms in Arabic). The second aspect is the AI language capability – ensuring the voice recognition and TTS work for new languages, and that the chatbot can understand queries in those languages. Llama-4 (the model we use) is already multilingual to an extent [2], but perhaps not fluent in all medical terminology across languages. We might need to integrate language-specific models for questions in other languages or force translate queries to English behind the scenes for the AI, then translate answers back to the user’s language. There are translation APIs which could be used (with caution regarding privacy). For voice, we’d add STT for those languages (most likely available through the OS or a cloud STT service) and TTS voices for them as well.
The benefit of expanded language support is making the app accessible in regions where English is not prevalent, thereby reducing language barriers to health information. For example, supporting Arabic would open usage in the Middle East; supporting Hindi could help users in India. Given the global reach of smartphones, this is a logical step for an app aiming at healthcare democratization. It’s also important for compliance and user comfort – being able to read health advice in one’s native language can improve comprehension and trust.
6.4 Personalized AI and Learning
With more usage data, Medicheck AI can evolve to provide more personalized insights. A future enhancement could include a personalization layer for the AI – for example, if the AI knows a user has diabetes and hypertension, it could tailor its general health advice to those conditions proactively. Or the app could learn the user’s baseline health metrics and alert when something deviates significantly. We also consider incorporating a reward mechanism or gamification to encourage healthy behavior, such as streaks for medication adherence or badges for maintaining normal blood pressure readings. While not directly a “research” feature, these can improve user engagement and outcomes, as many mHealth apps have shown through behavior change techniques.
6.5 Telemedicine Integration
In the future, Medicheck AI could integrate with telemedicine services – for instance, enabling a user to directly share their Medicheck AI data with a doctor or to initiate a tele-consultation if the AI suggests that a professional should be consulted. The app might have a “Consult a Doctor” button that connects to a network of physicians, perhaps via a partner platform. This would turn the app from just a self-service tool into a gateway for actual medical services when needed. Of course, this involves partnerships and ensuring data interoperability and consent.
6.6 Clinical Validation and Regulatory Approvals
To move from a project to a deployable health solution, we plan to conduct clinical validation studies. Future work includes rigorous testing of the AI’s advice against medical experts’ opinions to quantify its accuracy and safety. The outcomes would inform improvements and also support applications for regulatory approval, such as getting certified as a Medical Device Software in certain jurisdictions if we position it as such. This isn’t a feature per se, but a necessary step for real-world use. Through user studies and possibly trials, we’d gather evidence on efficacy, such as whether it improves medication adherence or prompts users to seek care appropriately, inspired by measures used in literature [3].
Each enhancement will require careful implementation to maintain the delicate balance of user trust, privacy, and usability. The modular architecture of Medicheck AI – with clear separation between frontend, backend, and AI modules – will facilitate these improvements. For example, adding a new wearable device might just mean a new Flutter plugin and some new data fields, without altering core logic. Adding new languages largely affects the UI/voice and somewhat the prompting for AI. Thus, the system is well-prepared for iterative growth
References:
1.	Deniz-Garcia, A., et al. (2023). Quality, usability, and effectiveness of mHealth apps and the role of artificial intelligence: Current scenario and challenges. Journal of Medical Internet Research, 25. https://doi.org/10.2196/45772
2.	Latif, S., et al. (2017). Mobile health in the developing world: Review of literature and lessons from a case study. IEEE Access, 5, 11540–11556.nature.com
3.	Zhang, X., et al. (2025). Accuracy of online symptom assessment applications, large language models, and laypeople for self-triage decisions. NPJ Digital Medicine, 8(124). https://www.nature.com/articles/s41746-025-00987-9
4.	Jung, L. B., et al. (2023). ChatGPT passes the German state examination in medicine with picture questions omitted. Deutsches Ärzteblatt International.
5.	Kung, T. H., et al. (2023). Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. PLOS Digital Health, 2(2), e0000198. https://doi.org/10.1371/journal.pdig.0000198
6.	Liu, M., et al. (2024). Performance of ChatGPT across different versions in medical licensing examinations worldwide: Systematic review and meta-analysis. Journal of Medical Internet Research, 26, e60807. https://doi.org/10.2196/60807
7.	Meta AI. (2024, September). Llama 3.2-Vision model card. Meta Platforms. https://huggingface.co/meta-llama/Llama-3.2-Vision
8.	Ollama. (2024, November 6). Llama 3.2 Vision is now available to run in Ollama. https://ollama.com/blog/llama-3.2-vision-release
9.	Groq Inc. (2024, April). Meta and Groq collaborate to deliver fast inference for the official Llama API [Press release]. https://groq.com/news/meta-groq-llama-inference
10.	Adam, G. (2024, May). Why Meta AI’s Llama 3 running on Groq’s LPU sets a new benchmark. Medium. https://medium.com/meta-ai-llama-groq-benchmark
11.	Pieoneers. (2023). Top 5 reasons to use Firebase as a backend for your health and wellness app. Pieoneers Tech Blog. https://pieoneers.com/blog/firebase-healthcare-apps
12.	Google Cloud. (2023). Firebase documentation – Security & compliance. https://firebase.google.com/docs/security
13.	Márquez, A. J., et al. (2020). Mobile apps for increasing treatment adherence: Systematic review. JMIR mHealth and uHealth, 8(10), e18812. https://doi.org/10.2196/18812
14.	Kowalski, J., et al. (2022). Voice-controlled intelligent personal assistants in health care: International Delphi study. Journal of Medical Internet Research, 24(3), e25642. https://doi.org/10.2196/25642
15.	Pradhan, A., et al. (2022). Hey Siri, help me take care of my child. A feasibility study using voice interaction for remote care management. Frontiers in Public Health, 10, 849322. https://doi.org/10.3389/fpubh.2022.849322

